# Code for my master's thesis: Deep Learning for Video Depth Estimation from Defocus

[Thesis Link](thesis/main_compressed.pdf)

## Thesis abstract

Depth estimation from RGB images is a challenging problem with many applications like 3D vision for robots or refocusing images after they have been recorded. Hardware solutions for recording depth like RGB-D cameras are often not applicable or it is desired to produce depth maps after the capture process. A way of predicting depth from RGB images is to use the amount of present defocus in a frame. Since the defocus blur degree is dependent on the object distance and the camera focus distance, it is possible to make assumptions on how far objects are away from the camera. To distinguish between blur and object texturing, multiple frames with different focus are used as input. Common approaches usually require the captured scene to be completely static making it impractical for dynamic video sequences. Our goal is thus to allow camera or object movement between the frames and eventually predict depth maps for arbitrary, continuous video sequences where the focus distance is always changed each frame. For this purpose, we are presenting several end-to-end trainable convolutional neural networks which can use the out-of-focus blur for making pixel-wise depth predictions for the displayed scene. Among the proposed networks, the recurrent autoencoder produces the best quality results. This type of network is a CNN which behaves similar to an LSTM but can handle multi-channel 2D images as input. Training CNNs typically demands a large dataset which has to fulfill special characteristics in our case. Since we are not aware of any existing dataset we create a labeled real-world dataset for training using an RGB-D camera and an Android smartphone. For that, we show the necessary calibration procedure and registration process to get aligned color and depth frames. Furthermore, we develop suitable Android applications which automatically change the focus every frame in a way that meets exactly the requirements for our CNN architectures. However, since manual recording is very time consuming and neural networks are known for needing a lot of data for training, we demonstrate how to efficiently create a synthetic dataset using the 3D graphics program Blender. Applying our approach, we can fully automatically produce large training datasets for our architectures. Trained on artificial data, our final models are nevertheless able to successfully generalize to real-world situations. Our solution, in the end, produces plausible depth map predictions even for challenging scenarios e.g. for settings under the influence of fast camera movement.
